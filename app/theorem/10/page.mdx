When adding two non-negative floating-point numbers $$x$$ and $$y$$, the relative error in computing $$x + y$$ is at most $$2\epsilon$$ (twice the [machine epsilon](/concept/epsilon)), even without using extra bits to protect against rounding errors.

This proof addresses the precision loss caused by adding two numbers, where the smaller number $$y$$ potentially loses digits when shifting to align with the larger one $$x$$. Even without guard digits, the shifted-off digits of $$y$$ contribute to a very small relative error in the full result, especially when $$x$$ is much larger than $$y$$. The rounding error from this shift, plus the final rounding to $$p$$ digits, leads to a maximum relative error of $$2\epsilon$$.

This theorem illustrates the surprisingly good accuracy of floating-point addition, even in the absence of guard digits, and reinforces the idea that the *structure of a formula* greatly affects its numerical stability.

## Theorem

If $$x \ge 0$$ and $$y \ge 0$$, the relative error in computing $$x + y$$ is at most $$2\epsilon$$, even when no guard digits are used.

## Proof

The algorithm for floating-point addition with $$k$$ guard digits closely resembles that used for subtraction. When $$x \ge y$$, we right-shift $$y$$ until its radix point aligns with that of $$x$$. Any digits shifted beyond the $$p + k$$-th place are discarded. We then compute the exact sum of these two truncated numbers (each with $$p + k$$ digits), and round the result to $$p$$ digits.

To analyze the worst-case scenario, it suffices to consider the case with no guard digits ($$k = 0$$). Without loss of generality, we may assume $$x \ge y \ge 0$$, and scale $$x$$ to the normalized form $$x = d.dd\ldots d \times \beta^0$$, where $$\beta$$ is the base of the floating-point system.

**Case 1: No carry-out during addition**  

In this case, the digits of $$y$$ that are shifted out contribute a value less than $$\beta^{-p + 1}$$. Since the sum $$x + y$$ is at least $$1$$, the relative error due to truncation is bounded by  

<Center>
    $$ \frac{\text{error}}{\text{true sum}} < \frac{\beta^{-p + 1}}{1} = \beta^{-p + 1} = 2\epsilon. $$
</Center>

**Case 2: Carry-out occurs**  

Here, we must account for both the truncation error and the rounding error. The rounding error is at most $$\frac{1}{2}\beta^{-p + 2}$$, and the truncation error is again at most $$\beta^{-p + 1}$$. Since the sum must now be at least $$\beta$$, the relative error is bounded by  

<Center>
$$ \frac{\beta^{-p + 1} + \frac{1}{2}\beta^{-p + 2}}{\beta} = \left(1 + \frac{\beta}{2}\right)\beta^{-p} \le 2\epsilon.$$
</Center>

In both cases, the relative error is bounded by $$2\epsilon$$. $$\blacksquare$$

## Discussion

This theorem provides part of the proof for [Theorem 2](/theorem/2), which establishes bounds for relative error in a single operation. These theorems collectively demonstrate why computing an expression like $$(x - y)(x + y)$$ is generally more accurate than computing $$x^2 - y^2$$, especially when $$x$$ and $$y$$ are close together. The latter can suffer large relative errors due to [catastrophic cancellation](/concept/cancellation), while the former maintains more numerical stability.