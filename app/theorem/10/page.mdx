## Theorem

If $$x \ge 0$$ and $$y \ge 0$$, then the relative error in computing $$x + y$$ is at most $$2\epsilon$$, even if no guard digits are used.

## Summary

This theorem states that when adding two non-negative floating-point numbers $$x$$ and $$y$$, the relative error in computing $$x + y$$ is at most $$ 2\epsilon$$, even without using extra bits to protect against rounding errors.

This proof addresses the precision loss caused by adding two numbers, where the smaller number $$y$$ to be shifted to align with the larger one $$x$$, potentially losing digits. Even without guard digits, the shifted-off digits of $$y$$ contribute to a very small relative error in the full result, especially when $$x$$ is much larger than $$y$$. The rounding error from this shift, plus the final rounding to $$p$$ digits, leads to a maximum relative error of $$2\epsilon$$.

This theorem illustrates the surprisingly good accuracy of floating-point addition - even in the absence of guard digits - and reinforces the idea that the *structure of a formula* greatly affects its numerical reliability.

## Proof

The algorithm for addition with $$k$$ guard digits is similar to that for subtraction. If $$x \ge y$$, shift $$y$$ right until the radix points of $$x$$ and $$y$$ are aligned. Discard any digits shifted past the $$p + k$$ position. Compute the sum of these two $$p + k$$ digit numbers exactly, then round to $$p$$ digits.

The general case of this theorem is similar to the case where no guard digits are used. There is no loss of generality in assuming that $$x \ge y \ge 0$$ and that $$x$$ is scaled to be of the form $$d.dd...d \times \beta^0$$. First, assume there is no carry out - the digits shifted off the end of $$y$$ have a value less than $$\beta^{-p + 1}$$, and the sum is at least $$1$$, so the relative error is less than $$\beta^{-p+1} = 2\epsilon$$. If there is a carry out, then the error from shifting must be added to the rounding error of $$\frac{\beta^{-p + 2}}{2}$$.

The sum is at least $$\beta$$, so the relative error is bounded as

<Center>
    $$\frac{\beta^{-p + 1} + \frac{1}{2}\beta^{-p + 2}}{\beta} = (1 + \frac{\beta}{2})\beta^{-p} \le 2\epsilon$$
</Center>

## Discussion

This theorem underpins [Theorem 2](/theorem/2), which provides bounds for relative error in a single operation. Combining Theorems 2 and 10 helps explain why computing an expression like $$(x - y)(x + y)$$ is generally more accurate than computing $$x^2 - y^2$$, especially when $$x$$ and $$y$$ are close together. The latter can suffer large relative errors due to [catastrophic cancellation](/concept/cancellation), while the former maintains more numerical stability.