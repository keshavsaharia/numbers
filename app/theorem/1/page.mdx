<Theorem id="1"/>

Subtracting two floating-point numbers using only $$p$$ digits (i.e. no guard digit) can result in a very large relative error, up to $$\beta - 1$$, where $$\beta$$ is the number base (typically 2 or 10). The worst case happens when you subtract a number like $$1.000\ldots0$$ from a very close number like $$0.\rho\rho\ldots\rho$$, where $$\rho = \beta - 1$$. Because there aren't enough digits to capture the fine difference, the subtraction loses key information and the result is much less accurate.

With base 10, this error could be 9 times the actual value, and in base 2, *every digit in the result could be incorrect* - a total failure of accuracy. To address this, floating-point systems use [guard digits](/concept/guard-digit) to keep extra precision during subtraction before rounding back to $$p$$ digits. This small change greatly improves accuracy, and allows the subtraction in the previous example to produce the exact result.

However, even with a guard digit, the relative error may occasionally be slightly larger than the standard rounding error $$\epsilon$$, as shown in an example where the result differs by about $$0.006$$ (compared to $$\epsilon = .005$$). Still, with a guard digit, the relative error stays small and controlled, which is why they are common in floating point systems.

## Theorem

In a floating-point system with base $$\beta$$ and precision $$p$$, the relative error in computing a difference using $$p$$ digits can be as large as $$\beta - 1$$.

## Proof

Consider two numbers:

$$x = 1.000\ldots0$$ (1 followed by $$p$$ zeros)  
$$y = 0.\rho\rho\ldots\rho$$ (with $$p$$ digits, each $$\rho = \beta - 1$$)

The exact difference is $$x - y = \beta^{-p}$$. 

However, when computed with only $$p$$ digits, the least significant digit of $$y$$ is lost, and the difference becomes $$\beta^{-p + 1}$$.

Therefore, the [absolute error](/concept/absolute-error) is $$\beta^{-p + 1} - \beta^{-p} = \beta^{-p}(\beta - 1)$$, and the [relative error](/concept/relative-error) is

$$ \frac{\beta^{-p}(\beta - 1)}{\beta^{-p}} = \beta - 1$$. &nbsp; $$\blacksquare$$ 

## Implications

With the base $$\beta = 2$$, the relative error in subtracting two close floating-point numbers can be as large as the result itself. For instance, a relative error of 100% ($$\beta - 1 = 1$$) means every significant digit in the computed result may be incorrect. For decimal systems ($$\beta = 10$$), the relative error can be as high as $$\beta - 1 = 9$$ - nine times the exact value!

To interpret this in terms of digit loss, consider this expression for the number of erroneous digits:  

$$ \log_\beta \left(\frac{1}{\text{relative error}}\right) $$  

Using our earlier example, when $$\beta = 2$$ and the relative error is $$1$$, we get:

$$ \log_2 \left(\frac{1}{1}\right) = 0 $$  

which implies that *none* of the digits in the result are reliable. This means that all $$p$$ digits can be corrupted when subtracting two nearly equal numbers.

## Guard Digits

To mitigate this problem, floating-point implementations often use a [guard digit](/concept/guard-digit) â€” an extra digit retained during intermediate steps of subtraction. This extra digit helps preserve accuracy when the result is small due to cancellation.

Consider a subtraction of $$x$$ and $$y$$ with a guard digit, where $$x = 1.010 \times 10^1$$ and $$y = 0.993 \times 10^1$$. The exact difference $$x - y = 0.017 \times 10^1 = 0.17$$ - because one extra digit was preserved before rounding, the result is computed exactly.

However, a guard digit doesn't guarantee perfect accuracy in all cases. In the case of $$x = 1.10 \times 10^2$$ and $$y = 0.085 \times 10^2$$, the exact difference is $$1.015 \times 10^2 = 101.5$$ and the rounded result is $$1.02 \times 10^2 = 102$$, yielding a relative error of $$\frac{102 - 101.5}{101.5} \approx 0.0049 $$ - slightly larger than [machine epsilon](/concept/epsilon) $$\epsilon = 0.005$$ for 2-digit precision in base 10.

<References reference={[
    'goldberg'
]}/>